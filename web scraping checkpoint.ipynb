{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9622a74b",
   "metadata": {},
   "source": [
    "## Create a Python script to automate data extraction from Wikipedia pages. The script will retrieve HTML content, extract article titles and text, collect internal links, and consolidate these tasks into one function that accepts a Wikipedia URL. This will be tested on a specific Wikipedia page to validate functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7255f3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af69fab",
   "metadata": {},
   "source": [
    "### 1. Write a function to Get and parse html content from a Wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91ac0957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki(page_link):\n",
    "    data = requests.get(page_link)\n",
    "    soup = BeautifulSoup(data.content, \"html.parser\")\n",
    "    return soup.prettify()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f26efd1",
   "metadata": {},
   "source": [
    "### 2. Write a function to Extract article title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb7f9b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(page_link):\n",
    "    art_title = get_wiki(page_link)\n",
    "    return art_title.title.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c392b6cb",
   "metadata": {},
   "source": [
    "### 3. Write a function to Extract article text for each paragraph with their respective headings. Map those headings to their respective paragraphs in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74877400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parag(page_link):\n",
    "    art_parag = get_wiki(page_link)\n",
    "    \n",
    "    content = {}\n",
    "    headings = 'Heading'\n",
    "    content[heading] = []\n",
    "    for tag in art_parag.find_all(['p', 'h1', 'h2', 'h3']):\n",
    "        if tag.name in ['h1', 'h2', 'h3']:\n",
    "            headings = tag.text.strip()\n",
    "        elif tag.name == 'p' and headings:\n",
    "            paragraph_text = tag.text.strip()\n",
    "            if paragraph_text:\n",
    "                content[heading].append(paragraph_text)\n",
    "        return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f022506",
   "metadata": {},
   "source": [
    "### 4. Write a function to collect every link that redirects to another Wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "177286c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def redirect(page_link):\n",
    "    for links in soup.find_all('a', href = True):\n",
    "        return links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf34d290",
   "metadata": {},
   "source": [
    "### 5. Wrap all the previous functions into a single function that takes as parameters a Wikipedia link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7be0fa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_link(page_link):\n",
    "    data = requests.get(page_link)\n",
    "    soup = BeautifulSoup(data.content, \"html.parser\")\n",
    "    art_title = soup(page_link)\n",
    "    content = {}\n",
    "    headings = 'Heading'\n",
    "    content[headings] = []\n",
    "    for tag in soup.find_all(['p', 'h1', 'h2', 'h3']):\n",
    "        if tag.name in ['h1', 'h2', 'h3']:\n",
    "            headings = tag.text.strip()\n",
    "        elif tag.name == 'p' and headings:\n",
    "            paragraph_text = tag.text.strip()\n",
    "    for links in soup.find_all('a', href = True):\n",
    "        return links\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "68e37ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"mw-jump-link\" href=\"#bodyContent\">Jump to content</a>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_link('https://en.wikipedia.org/wiki/Lagos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedf433e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
